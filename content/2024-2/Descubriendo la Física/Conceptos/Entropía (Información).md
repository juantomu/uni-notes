# Entropía (Información)
***
La entropía también se puede analizar como la cantidad de información que se sabe de un sistema.

Mucha información --> Más orden --> Menor entropía.
Poca información --> Más desorden --> Mayor entropía.

Las configuraciones completamente ordenadas son muy pocas, mientras que las configuraciones desordenadas son muchas. Por probabilidad es más probable que un sistema se encuentre en una configuración desordenada.

En la naturaleza los sistemas físicos evolucionan de forma natural(con mayor probabilidad) a estados de mayor entropía, es decir, la entropía siempre aumenta. En la naturaleza los sistemas tienden (con mayor probabilidad) al desorden y el caos.